# ğŸ“˜ Online Inference API Documentation

This document describes the **design, behavior, and usage** of the
**FastAPI-based online inference service** used to serve the trained
California Housing price prediction model.

The API is built as a **thin, production-ready serving layer** on top of
validated ML pipelines and artifacts.

## ğŸ¯ Purpose of the API

The API provides:

* **Real-time predictions** for housing prices
* A stable interface over **production ML artifacts**
* A deployment-ready service suitable for CI/CD and containerized environments

The API **does not train models** and **does not perform experimentation**.
It strictly handles **inference**.

## ğŸ—ï¸ High-Level Architecture

```
Client Request
   â†“
FastAPI Route (/predict)
   â†“
Request Validation (Pydantic)
   â†“
Shared Preprocessing Logic
   â†“
Loaded Production Model
   â†“
Prediction Response
```

Key design goal:

> **The preprocessing and inference logic used here is identical to training.**

## ğŸš€ Application Lifecycle

### Artifact Loading (Startup)

Model artifacts are loaded **once at application startup** using FastAPIâ€™s
modern **lifespan** mechanism:

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    ARTIFACTS = load_prod_artifacts(...)
```

Loaded artifacts include:

* Trained regression model
* Imputer
* Encoder
* Any required preprocessing objects

These artifacts are:

* Stored in memory
* Read-only
* Shared across all requests

This avoids:

* Per-request loading
* Race conditions
* Performance overhead


## ğŸ“¡ API Endpoints

### `GET /health`

**Purpose:**
Health check endpoint for:

* Docker
* CI pipelines
* Load balancers

**Response:**

```json
{
  "status": "ok"
}
```

---

### `POST /predict`

**Purpose:**
Run real-time housing price predictions.

---

### Request Schema

The request accepts a **batch of records**:

```json
{
  "data": [
    {
      "longitude": -122.23,
      "latitude": 37.88,
      "housing_median_age": 41,
      "total_rooms": 880,
      "total_bedrooms": 129,
      "population": 322,
      "households": 126,
      "median_income": 8.3252,
      "ocean_proximity": "NEAR BAY"
    }
  ]
}
```

Validation is enforced using **Pydantic models** to ensure:

* Correct data types
* Required fields
* Early rejection of invalid inputs

---

### Response Schema

```json
{
  "predictions": [452310.25]
}
```

Each prediction corresponds to one input record.


## ğŸ” Inference Flow (Step-by-Step)

Inside the `/predict` route:

1. **Input validation**

   * JSON â†’ Pydantic models
2. **Data conversion**

   * Pydantic â†’ Pandas DataFrame
3. **Artifact availability check**

   * Ensures model and preprocessors are loaded
4. **Preprocessing**

   * Imputation
   * Encoding
   * Feature engineering
5. **Model inference**

   * Uses the production regression model
6. **Response serialization**

   * Predictions returned as JSON

The route contains **no ML logic itself** â€” it orchestrates existing components.

## ğŸ§  Relationship to ML Code (`src/`)

It **reuses**:

* `src.preprocessing`
* `src.inference`
* Production artifacts generated by training pipelines


## ğŸªµ Logging Strategy

The API uses **structured file-based logging**:

* API startup and shutdown
* Artifact loading
* Request receipt and batch size
* Successful inference
* Errors and stack traces

Logs are written to:

```
logs/api.log
```

Only **high-signal events** are logged â€” input payloads and raw predictions
are intentionally excluded.


## ğŸ§ª Testing

The API includes automated tests for:

* Health endpoint
* Successful prediction requests
* Invalid input handling

Tests use FastAPIâ€™s `TestClient` and validate:

* Correct status codes
* Schema enforcement
* Error handling


## ğŸ“¦ Containerization

The inference service is fully containerized using Docker:

* Lightweight base image
* API-only dependencies
* Production artifacts bundled
* Stateless runtime

The container exposes port `8000` and runs via `uvicorn`.

---