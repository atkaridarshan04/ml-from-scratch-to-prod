# ML from Scratch to Production (API Baseline)

This branch contains the **API baseline implementation**, where the online
inference service loads trained models and preprocessing artifacts directly
from the local filesystem (`artifacts/production`).

It represents the system state **before introducing MLflow-based model
registry and lifecycle management**, and is kept as a stable reference for
comparison with the production MLOps design in the `main` branch.


## ğŸ¯ What This Branch Represents

This branch contains a **pre-MLflow production-style system**, including:

- Finalized ML training pipeline
- Batch inference pipeline
- FastAPI-based online inference service
- Local artifact management (`artifacts/production`)
- Shared preprocessing logic across training, batch inference, and API
- Automated API tests
- Dockerized inference service

This design serves as a **baseline architecture** before adopting a centralized
model registry and lifecycle management.



## ğŸ§  Machine Learning Overview

- Multiple model families were evaluated during experimentation
- Feature engineering was validated across models
- **Gradient Boosting (`HistGradientBoostingRegressor`)** achieved the best
  generalization performance
- This model was selected as the **production baseline**
- Finalized preprocessing and modeling logic was migrated into Python pipelines

### Detailed ML artifacts are available in:
- `notebooks/` â€” experimentation and EDA
- `docs/` â€” ML design, feature analysis, and modeling decisions
- `artifacts/experiments/` â€” historical experiment outputs

> **For only ml workflow refer the `ml-baseline` branch**



## ğŸ—‚ï¸ Repository Structure (API Baseline)

```
root
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ experiments/         # Historical experiment outputs
â”‚   â””â”€â”€ production/          # Deployment-ready ML artifacts
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original dataset
â”‚   â””â”€â”€ inference/           # Inference inputs & generators
â”œâ”€â”€ docs/                    # ML design & decision records
â”œâ”€â”€ notebooks/               # Experimentation history
â”œâ”€â”€ outputs/                 # Batch inference outputs
â”œâ”€â”€ pipelines/               # Training & batch inference entry points
â”œâ”€â”€ requirements/            # Dependency split (train / api)
â”œâ”€â”€ src/                     # Production ML & API code
â”œâ”€â”€ tests/                   # API tests
â”œâ”€â”€ Dockerfile               # Inference service containerization
â””â”€â”€ README.md
```


## âš™ï¸ ML Pipelines

### Create and activate a virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### Install API dependencies

```bash
python -m pip install --upgrade pip
pip install -r requirements/train.txt

export PYTHONPATH=$(pwd)/src
```

### Training Pipeline

```bash
python -m pipelines.train
```

Responsibilities:

* Loads raw dataset
* Applies preprocessing and feature engineering
* Trains the final Gradient Boosting model
* Evaluates performance
* Saves trained artifacts locally

Artifacts are written to:

```
artifacts/production/
```

---

### Batch Inference Pipeline

```bash
python -m pipelines.inference
```

Responsibilities:

* Loads production artifacts from the filesystem
* Applies identical preprocessing as training
* Runs predictions on inference input data

Outputs are written to:

```
outputs/predictions.json
```

Sample inference data can be generated using:

```bash
python data/inference/generate_sample.py
```



## ğŸŒ Online Inference API

This branch exposes a **FastAPI-based online inference service** for real-time
housing price predictions.

### API Characteristics

* FastAPI REST service
* Request/response validation using Pydantic
* Artifact loading from `artifacts/production` at startup
* Shared preprocessing logic with training & batch inference
* Structured file-based logging
* Automated API tests
* Dockerized for deployment

### Available Endpoints

* `GET /health` â€” health check
* `POST /predict` â€” run housing price predictions



## â–¶ï¸ Running the API Locally (Python Environment)

### 1ï¸âƒ£ Install API dependencies

```bash
pip install -r requirements/api.txt
```

### 2ï¸âƒ£ Set Python path
```bash
export PYTHONPATH=$(pwd)/src
```

### 3ï¸âƒ£ Start the API server

```bash
uvicorn api.main:app --reload
```

* API: [http://localhost:8000](http://localhost:8000)
* Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)


## ğŸ³ Running the API with Docker

```bash
docker build -t housing-api .
docker run -p 8000:8000 housing-api
```


## ğŸ§ª Running Tests

```bash
pytest -v
```

## ğŸ“Œ Note

This branch represents a **filesystem-based API design** and is intentionally
kept as a stable reference point.

The **current production-grade MLOps implementation**, using MLflow for model
registry and lifecycle management, is available in the `main` branch.


---